{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## AI Bot: Transcription → Notes → Summary (Gradio + Jupyter)\n\nThis notebook builds a simple, modular AI bot that:\n- Records or uploads audio\n- Transcribes the audio to text\n- Generates detailed notes and topic-wise summaries using an LLM\n\nWe use open-source tools (`faster-whisper` for transcription, Hugging Face `transformers` for summarization) and optionally OpenAI for stronger summaries. Each section is modular in its own cell.\n\n**Requirements**: Ensure FFmpeg is installed on your system for audio decoding. On Debian/Ubuntu:\n\n```bash\nsudo apt-get update && sudo apt-get install -y ffmpeg\n```\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Setup / Imports\nInstall required packages and import libraries. If OpenAI is available and you set `OPENAI_API_KEY`, the summarizer can use it; otherwise it falls back to local models.\n\nIf running for the first time, uncomment the `%pip install` lines below to install dependencies."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running first time, uncomment to install\n",
        "# %pip install -q gradio==4.* faster-whisper==1.* transformers==4.* torch --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "# Optional: OpenAI for summarization via API\n",
        "# %pip install -q openai==1.* tiktoken==0.*\n",
        "\n",
        "import os\n",
        "from typing import Optional, Tuple, Dict, Union, List\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Transcription\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "# Summarization (local)\n",
        "from transformers import pipeline\n",
        "\n",
        "# Optional: OpenAI\n",
        "try:\n",
        "    from openai import OpenAI  # requires openai>=1.0\n",
        "    OPENAI_AVAILABLE = True\n",
        "except Exception:\n",
        "    OPENAI_AVAILABLE = False\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
        "USE_OPENAI = OPENAI_AVAILABLE and bool(OPENAI_API_KEY)\n",
        "print('OpenAI enabled:', USE_OPENAI)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Audio Recording / Upload\nWe will use Gradio's `Audio` component to record audio in the browser or upload files. We'll configure it to return a local file path which is then passed to the transcription function."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradio Audio configuration\n",
        "AUDIO_SOURCES = ['microphone', 'upload']\n",
        "AUDIO_TYPE = 'filepath'  # Gradio will pass a file path for the audio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Transcription Function (Whisper)\nWe use `faster-whisper` for CPU/GPU-friendly inference. Select a model size like `small`, `medium`, or `large-v3` via the `WHISPER_MODEL` environment variable. The function accepts a file path (from Gradio) or raw bytes and returns the transcribed text."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a global Whisper model lazily to avoid repeated loads.\n",
        "_WHISPER_MODEL_NAME = os.environ.get('WHISPER_MODEL', 'small')\n",
        "_WHISPER_DEVICE = os.environ.get('WHISPER_DEVICE', 'cpu')  # 'cpu' or 'cuda'\n",
        "_whisper_model: Optional[WhisperModel] = None\n",
        "\n",
        "def get_whisper_model() -> WhisperModel:\n",
        "    global _whisper_model\n",
        "    if _whisper_model is None:\n",
        "        compute_type = 'int8' if _WHISPER_DEVICE == 'cpu' else 'float16'\n",
        "        _whisper_model = WhisperModel(_WHISPER_MODEL_NAME, device=_WHISPER_DEVICE, compute_type=compute_type)\n",
        "    return _whisper_model\n",
        "\n",
        "def transcribe_audio(audio_file_or_bytes: Union[bytes, str]) -> str:\n",
        "    \"\"\"Transcribe audio using faster-whisper. Accepts a file path or raw bytes.\n",
        "    Returns the full transcript as a string.\n",
        "    \"\"\"\n",
        "    model = get_whisper_model()\n",
        "\n",
        "    import tempfile, os as _os\n",
        "    tmp_path = None\n",
        "    if isinstance(audio_file_or_bytes, (bytes, bytearray)):\n",
        "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
        "            tmp.write(audio_file_or_bytes)\n",
        "            tmp_path = tmp.name\n",
        "    else:\n",
        "        tmp_path = audio_file_or_bytes\n",
        "\n",
        "    segments, info = model.transcribe(tmp_path, beam_size=5, vad_filter=True)\n",
        "    text_parts: List[str] = []\n",
        "    for seg in segments:\n",
        "        text_parts.append(seg.text)\n",
        "    transcript = ' '.join(part.strip() for part in text_parts if part.strip())\n",
        "\n",
        "    # Clean up temp file if created\n",
        "    if isinstance(audio_file_or_bytes, (bytes, bytearray)) and tmp_path and _os.path.exists(tmp_path):\n",
        "        try:\n",
        "            _os.remove(tmp_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return transcript\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### LLM Summary and Notes Function\nTwo backends are provided:\n- Local: Hugging Face `transformers` summarization pipeline (default `facebook/bart-large-cnn`).\n- OpenAI (optional): If `OPENAI_API_KEY` is set, use GPT for stronger summaries and topic breakdowns.\n\nThe function returns three strings:\n- Raw transcription text\n- Clean summarized notes\n- Topic-wise breakdown (Markdown headings)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "_LOCAL_SUMMARY_MODEL = os.environ.get('SUMMARY_MODEL', 'facebook/bart-large-cnn')\n",
        "_local_summarizer = None\n",
        "\n",
        "def get_local_summarizer():\n",
        "    global _local_summarizer\n",
        "    if _local_summarizer is None:\n",
        "        _local_summarizer = pipeline(\n",
        "            'summarization',\n",
        "            model=_LOCAL_SUMMARY_MODEL,\n",
        "            tokenizer=_LOCAL_SUMMARY_MODEL,\n",
        "            truncation=True\n",
        "        )\n",
        "    return _local_summarizer\n",
        "\n",
        "_SYSTEM_PROMPT = (\n",
        "    'You are an expert note-taker. Given a meeting transcript, '\n",
        "    'produce: 1) concise bullet notes, 2) topic-wise breakdown with headings, '\n",
        "    '3) explicit action items and decisions if present. Keep factual, clear.'\n",
        ")\n",
        "\n",
        "def summarize_with_openai(transcript: str) -> Tuple[str, str]:\n",
        "    client = OpenAI()\n",
        "    user_prompt = (\n",
        "        'Transcript:\\n\\n' + transcript + '\\n\\n'\n",
        "        'Return exactly two sections in Markdown with these exact headers:\\n'\n",
        "        'NOTES:\\n- bullet points...\\n\\n'\n",
        "        'TOPICS:\\n### Title... with brief summaries.'\n",
        "    )\n",
        "    completion = client.chat.completions.create(\n",
        "        model=os.environ.get('OPENAI_MODEL', 'gpt-4o-mini'),\n",
        "        messages=[\n",
        "            {'role': 'system', 'content': _SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': user_prompt},\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    content = completion.choices[0].message.content or ''\n",
        "    m = re.search(r'(?is)NOTES\\s*:\\s*(.*?)\\n\\s*TOPICS\\s*:\\s*(.*)$', content, re.DOTALL)\n",
        "    if m:\n",
        "        notes = m.group(1).strip()\n",
        "        topics = m.group(2).strip()\n",
        "    else:\n",
        "        notes = content.strip()\n",
        "        topics = content.strip()\n",
        "    return notes, topics\n",
        "\n",
        "def summarize_with_local_model(transcript: str) -> Tuple[str, str]:\n",
        "    summarizer = get_local_summarizer()\n",
        "    # Chunk long transcripts to fit model limits (simple heuristic by characters)\n",
        "    chunks: List[str] = []\n",
        "    curr: List[str] = []\n",
        "    curr_len = 0\n",
        "    for token in transcript.split():\n",
        "        tlen = len(token) + 1\n",
        "        if curr_len + tlen > 3500:\n",
        "            chunks.append(' '.join(curr))\n",
        "            curr = [token]\n",
        "            curr_len = tlen\n",
        "        else:\n",
        "            curr.append(token)\n",
        "            curr_len += tlen\n",
        "    if curr:\n",
        "        chunks.append(' '.join(curr))\n",
        "\n",
        "    summaries: List[str] = []\n",
        "    for ch in chunks:\n",
        "        out = summarizer(ch, max_length=256, min_length=64, do_sample=False)\n",
        "        summaries.append(out[0]['summary_text'])\n",
        "\n",
        "    combined = '\\n\\n'.join(summaries)\n",
        "\n",
        "    # Build notes and topics using a simple template from combined summary\n",
        "    notes_lines = [f'- {line.strip()}' for line in combined.split('. ') if line.strip()]\n",
        "    notes = '\\n'.join(notes_lines)\n",
        "\n",
        "    # Naive topic split; real systems may use topic modeling\n",
        "    topics = '\\n\\n'.join(\n",
        "        [f'### Topic {i+1}\\n{para.strip()}' for i, para in enumerate(combined.split('\\n\\n')) if para.strip()]\n",
        "    )\n",
        "    return notes, topics\n",
        "\n",
        "def generate_notes_and_topics(transcript: str) -> Tuple[str, str, str]:\n",
        "    if not transcript or not transcript.strip():\n",
        "        return '', '', ''\n",
        "    if USE_OPENAI:\n",
        "        notes, topics = summarize_with_openai(transcript)\n",
        "    else:\n",
        "        notes, topics = summarize_with_local_model(transcript)\n",
        "    return transcript, notes, topics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Gradio Interface\nA simple UI allowing you to record or upload audio, view the transcription, and generate notes plus a topic-wise breakdown."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradio_process(audio: Union[str, None]) -> Tuple[str, str, str]:\n",
        "    \"\"\"audio is a filepath from the upload/recorder or None.\n",
        "    Returns (transcript, notes, topics).\n",
        "    \"\"\"\n",
        "    if not audio:\n",
        "        return '', '', ''\n",
        "    transcript = transcribe_audio(audio)\n",
        "    t, notes, topics = generate_notes_and_topics(transcript)\n",
        "    return t, notes, topics\n",
        "\n",
        "with gr.Blocks(title='Transcribe → Notes → Topics') as demo:\n",
        "    gr.Markdown(\n",
        "        '## Transcription → Notes → Topics\\n'\n",
        "        'Record or upload audio. The app will transcribe it and produce notes and a topic-wise breakdown.'\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        audio_in = gr.Audio(sources=AUDIO_SOURCES, type=AUDIO_TYPE, label='Record or Upload Audio')\n",
        "    btn = gr.Button('Transcribe and Summarize')\n",
        "\n",
        "    with gr.Row():\n",
        "        out_transcript = gr.Textbox(label='Transcript', lines=12)\n",
        "    with gr.Row():\n",
        "        out_notes = gr.Markdown()\n",
        "    with gr.Row():\n",
        "        out_topics = gr.Markdown()\n",
        "\n",
        "    btn.click(fn=gradio_process, inputs=[audio_in], outputs=[\n",
        "        out_transcript, out_notes, out_topics\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Demo: Launch the App\nRun the cell below to launch the Gradio app. Set `share=True` to get a public URL." 
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running remotely, you can share via `share=True`.\n",
        "demo.launch(share=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
